#!/usr/bin/env python3
"""
Autonomous QA Testing Council MCP Server (FIXED VERSION)
Multi-agent system for test generation, execution, and repair

FIXES APPLIED:
1. Fixed file path resolution for Docker containers
2. Improved orchestrator to actually call sub-agents
3. Added proper error handling and retries
4. Fixed directory existence checks
5. Added GitHub API integration for PR creation
"""
import os
import sys
import logging
import json
import subprocess
import shutil
import ast
import re
from pathlib import Path
from datetime import datetime, timezone
from mcp.server.fastmcp import FastMCP
import httpx

# Configure logging to stderr
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    stream=sys.stderr
)
logger = logging.getLogger("qa-council-server")

# Initialize MCP server
mcp = FastMCP("qa-council")

# Configuration
GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN", "")
WORKSPACE_DIR = Path("/app/repos")
TEST_RESULTS_DIR = Path("/app/test_results")
COVERAGE_DIR = Path("/app/coverage")

# Ensure directories exist
WORKSPACE_DIR.mkdir(exist_ok=True)
TEST_RESULTS_DIR.mkdir(exist_ok=True)
COVERAGE_DIR.mkdir(exist_ok=True)

# === UTILITY FUNCTIONS ===

def sanitize_repo_name(repo_url: str) -> str:
    """Extract safe directory name from repo URL."""
    name = repo_url.rstrip('/').split('/')[-1]
    if name.endswith('.git'):
        name = name[:-4]
    return re.sub(r'[^a-zA-Z0-9_-]', '_', name)

def extract_github_info(repo_url: str) -> tuple:
    """Extract owner and repo name from GitHub URL."""
    parts = repo_url.rstrip('/').split('/')
    if 'github.com' in repo_url:
        owner = parts[-2]
        repo = parts[-1].replace('.git', '')
        return owner, repo
    return None, None

def clone_or_update_repo(repo_url: str, branch: str = "main") -> tuple:
    """Clone or update a GitHub repository."""
    repo_name = sanitize_repo_name(repo_url)
    repo_path = WORKSPACE_DIR / repo_name
    
    try:
        if repo_path.exists():
            logger.info(f"Updating existing repo: {repo_name}")
            result = subprocess.run(
                ["git", "-C", str(repo_path), "pull", "origin", branch],
                capture_output=True,
                text=True,
                timeout=60
            )
            if result.returncode != 0:
                return False, f"Git pull failed: {result.stderr}"
        else:
            logger.info(f"Cloning new repo: {repo_name}")
            git_url = repo_url
            if GITHUB_TOKEN:
                git_url = repo_url.replace("https://", f"https://{GITHUB_TOKEN}@")
            
            result = subprocess.run(
                ["git", "clone", "-b", branch, git_url, str(repo_path)],
                capture_output=True,
                text=True,
                timeout=120
            )
            if result.returncode != 0:
                return False, f"Git clone failed: {result.stderr}"
        
        return True, str(repo_path)
    except subprocess.TimeoutExpired:
        return False, "Git operation timed out"
    except Exception as e:
        return False, str(e)

def verify_path_exists(path: str) -> tuple:
    """Verify path exists and is accessible - FIXED VERSION."""
    try:
        p = Path(path)
        # Try multiple verification methods
        if p.exists():
            return True, str(p)
        
        # Try using os.path.exists as fallback
        if os.path.exists(path):
            return True, path
        
        # Try listing parent directory to verify
        parent = p.parent
        if parent.exists():
            children = list(parent.iterdir())
            for child in children:
                if child.name == p.name:
                    return True, str(child)
        
        return False, f"Path not found: {path}"
    except Exception as e:
        return False, f"Path verification error: {str(e)}"

def analyze_python_file(file_path: str) -> dict:
    """Analyze Python file structure and extract testable components."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        tree = ast.parse(content)
        
        functions = []
        classes = []
        imports = []
        
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                functions.append({
                    "name": node.name,
                    "args": [arg.arg for arg in node.args.args],
                    "lineno": node.lineno,
                    "is_async": isinstance(node, ast.AsyncFunctionDef)
                })
            elif isinstance(node, ast.ClassDef):
                methods = [n.name for n in node.body if isinstance(n, ast.FunctionDef)]
                classes.append({
                    "name": node.name,
                    "methods": methods,
                    "lineno": node.lineno
                })
            elif isinstance(node, ast.Import):
                imports.extend([alias.name for alias in node.names])
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.append(node.module)
        
        return {
            "functions": functions,
            "classes": classes,
            "imports": imports,
            "total_lines": len(content.split('\n'))
        }
    except Exception as e:
        logger.error(f"Error analyzing {file_path}: {e}")
        return {"error": str(e)}

def generate_unit_test(component: dict, file_path: str) -> str:
    """Generate unit test code for a component."""
    if "name" in component:
        if "methods" in component:
            test_code = f'''"""Unit tests for {component["name"]} class."""
import pytest
from {Path(file_path).stem} import {component["name"]}

class Test{component["name"]}:
    """Test suite for {component["name"]}."""
    
    def setup_method(self):
        """Setup test fixtures."""
        self.instance = {component["name"]}()
    
'''
            for method in component["methods"]:
                if not method.startswith("_"):
                    test_code += f'''    def test_{method}(self):
        """Test {method} method."""
        # TODO: Add test implementation
        assert hasattr(self.instance, "{method}")
    
'''
        else:
            test_code = f'''"""Unit tests for {component["name"]} function."""
import pytest
from {Path(file_path).stem} import {component["name"]}

def test_{component["name"]}_basic():
    """Test basic functionality of {component["name"]}."""
    # TODO: Add test implementation
    pass

def test_{component["name"]}_edge_cases():
    """Test edge cases for {component["name"]}."""
    # TODO: Add test implementation
    pass
'''
        return test_code
    return ""

def generate_playwright_test(page_url: str, test_name: str) -> str:
    """Generate Playwright E2E test."""
    return f'''"""E2E tests for {test_name}."""
import pytest
from playwright.sync_api import Page, expect
import re

def test_{test_name}_page_loads(page: Page, base_url: str):
    """Test that the page loads successfully."""
    page.goto(base_url)
    expect(page).to_have_title(re.compile(r".+"))

def test_{test_name}_navigation(page: Page, base_url: str):
    """Test navigation elements."""
    page.goto(base_url)
    # TODO: Add navigation tests

def test_{test_name}_interactions(page: Page, base_url: str):
    """Test user interactions."""
    page.goto(base_url)
    # TODO: Add interaction tests
'''

def run_pytest(repo_path: str, test_path: str = "") -> tuple:
    """Execute pytest with coverage."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = TEST_RESULTS_DIR / f"report_{timestamp}.html"
    coverage_file = COVERAGE_DIR / f"coverage_{timestamp}.xml"
    
    cmd = [
        "pytest",
        "-v",
        "--tb=short",
        f"--html={report_file}",
        "--self-contained-html",
        f"--cov={repo_path}",
        f"--cov-report=xml:{coverage_file}",
        "--cov-report=term"
    ]
    
    if test_path:
        cmd.append(test_path)
    else:
        cmd.append(repo_path)
    
    try:
        result = subprocess.run(
            cmd,
            cwd=repo_path,
            capture_output=True,
            text=True,
            timeout=300
        )
        
        return True, {
            "exit_code": result.returncode,
            "stdout": result.stdout,
            "stderr": result.stderr,
            "report_file": str(report_file),
            "coverage_file": str(coverage_file)
        }
    except subprocess.TimeoutExpired:
        return False, {"error": "Test execution timed out"}
    except Exception as e:
        return False, {"error": str(e)}

def parse_test_failures(pytest_output: str) -> list:
    """Parse pytest output to extract failure information."""
    failures = []
    lines = pytest_output.split('\n')
    
    current_failure = {}
    in_failure = False
    
    for line in lines:
        if line.startswith("FAILED"):
            in_failure = True
            current_failure = {"test": line.split()[0], "lines": []}
        elif in_failure:
            if line.startswith("===") or line.startswith("PASSED") or line.startswith("FAILED"):
                if current_failure.get("lines"):
                    failures.append(current_failure)
                current_failure = {}
                in_failure = False
            else:
                current_failure.get("lines", []).append(line)
    
    return failures

def generate_test_repair(failure_info: dict) -> str:
    """Generate suggestions for repairing failed tests."""
    suggestions = []
    
    failure_text = '\n'.join(failure_info.get("lines", []))
    
    if "AssertionError" in failure_text:
        suggestions.append("Check assertion conditions - expected vs actual values may not match")
    if "AttributeError" in failure_text:
        suggestions.append("Verify object attributes and method names are correct")
    if "TypeError" in failure_text:
        suggestions.append("Check function argument types and counts")
    if "ImportError" in failure_text or "ModuleNotFoundError" in failure_text:
        suggestions.append("Ensure all required modules are installed and import paths are correct")
    if "fixture" in failure_text.lower():
        suggestions.append("Verify pytest fixtures are properly defined and scoped")
    
    if not suggestions:
        suggestions.append("Review test logic and ensure it matches current implementation")
    
    return suggestions

async def create_github_pr(owner: str, repo: str, title: str, body: str, head_branch: str, base_branch: str = "main") -> tuple:
    """Create a GitHub Pull Request with recommended changes."""
    if not GITHUB_TOKEN:
        return False, "GitHub token not configured"
    
    url = f"https://api.github.com/repos/{owner}/{repo}/pulls"
    headers = {
        "Authorization": f"token {GITHUB_TOKEN}",
        "Accept": "application/vnd.github.v3+json"
    }
    
    data = {
        "title": title,
        "body": body,
        "head": head_branch,
        "base": base_branch
    }
    
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(url, headers=headers, json=data, timeout=30)
            if response.status_code == 201:
                pr_data = response.json()
                return True, pr_data["html_url"]
            else:
                return False, f"GitHub API error: {response.status_code} - {response.text}"
    except Exception as e:
        return False, f"Error creating PR: {str(e)}"

async def create_test_fix_branch(repo_path: str, branch_name: str, fixes: list) -> tuple:
    """Create a new branch with test fixes."""
    try:
        # Create new branch
        result = subprocess.run(
            ["git", "-C", repo_path, "checkout", "-b", branch_name],
            capture_output=True,
            text=True,
            timeout=10
        )
        
        if result.returncode != 0:
            return False, f"Failed to create branch: {result.stderr}"
        
        # Apply fixes (write fix files)
        for fix in fixes:
            file_path = Path(repo_path) / fix["file"]
            file_path.parent.mkdir(parents=True, exist_ok=True)
            with open(file_path, 'w') as f:
                f.write(fix["content"])
        
        # Stage changes
        subprocess.run(["git", "-C", repo_path, "add", "."], check=True, timeout=10)
        
        # Commit
        subprocess.run(
            ["git", "-C", repo_path, "commit", "-m", "fix: Apply automated test repairs from QA Council"],
            check=True,
            timeout=10
        )
        
        # Push
        subprocess.run(
            ["git", "-C", repo_path, "push", "-u", "origin", branch_name],
            capture_output=True,
            text=True,
            timeout=30
        )
        
        return True, branch_name
    except Exception as e:
        return False, f"Error creating fix branch: {str(e)}"

# === MCP TOOLS ===

@mcp.tool()
async def clone_repository(repo_url: str = "", branch: str = "main") -> str:
    """Clone or update a GitHub repository for testing."""
    if not repo_url.strip():
        return "âŒ Error: Repository URL is required"
    
    logger.info(f"Cloning repository: {repo_url}")
    
    success, result = clone_or_update_repo(repo_url, branch)
    
    if success:
        return f"âœ… Repository ready at: {result}"
    else:
        return f"âŒ Repository clone failed: {result}"

@mcp.tool()
async def analyze_codebase(repo_path: str = "", file_pattern: str = "*.py") -> str:
    """Analyze Python codebase structure and identify testable components."""
    if not repo_path.strip():
        return "âŒ Error: Repository path is required"
    
    # FIXED: Verify path exists
    path_exists, verified_path = verify_path_exists(repo_path)
    if not path_exists:
        return f"âŒ Error: {verified_path}"
    
    path = Path(verified_path)
    logger.info(f"Analyzing codebase at: {verified_path}")
    
    try:
        # Use os.walk for better compatibility
        python_files = []
        for root, dirs, files in os.walk(verified_path):
            # Skip common ignore directories
            dirs[:] = [d for d in dirs if d not in ['__pycache__', '.git', 'venv', 'env', 'node_modules']]
            for file in files:
                if file.endswith('.py') and not file.startswith('test_'):
                    python_files.append(os.path.join(root, file))
        
        if not python_files:
            return f"âš ï¸ No Python files found matching pattern: {file_pattern}"
        
        analysis = {
            "total_files": len(python_files),
            "files": []
        }
        
        for py_file in python_files[:50]:
            file_analysis = analyze_python_file(py_file)
            file_analysis["path"] = py_file
            analysis["files"].append(file_analysis)
        
        total_functions = sum(len(f.get("functions", [])) for f in analysis["files"])
        total_classes = sum(len(f.get("classes", [])) for f in analysis["files"])
        
        result = f"""ğŸ“Š Codebase Analysis Complete

ğŸ“ Files analyzed: {analysis['total_files']}
âš¡ Functions found: {total_functions}
ğŸ—ï¸ Classes found: {total_classes}

Top files for testing:
"""
        for i, file_info in enumerate(analysis["files"][:10], 1):
            if "error" not in file_info:
                result += f"\n{i}. {Path(file_info['path']).name}"
                result += f"\n   - Functions: {len(file_info['functions'])}"
                result += f"\n   - Classes: {len(file_info['classes'])}"
        
        return result
        
    except Exception as e:
        logger.error(f"Analysis error: {e}")
        return f"âŒ Error analyzing codebase: {str(e)}"

@mcp.tool()
async def generate_unit_tests(repo_path: str = "", target_file: str = "") -> str:
    """Generate unit tests for Python functions and classes in a file."""
    if not repo_path.strip():
        return "âŒ Error: Repository path is required"
    if not target_file.strip():
        return "âŒ Error: Target file path is required"
    
    logger.info(f"Generating unit tests for: {target_file}")
    
    # FIXED: Better path resolution
    path_exists, verified_path = verify_path_exists(repo_path)
    if not path_exists:
        return f"âŒ Error: Repository path issue - {verified_path}"
    
    file_path = Path(verified_path) / target_file
    
    if not file_path.exists():
        return f"âŒ Error: File not found: {target_file}"
    
    try:
        analysis = analyze_python_file(str(file_path))
        
        if "error" in analysis:
            return f"âŒ Error analyzing file: {analysis['error']}"
        
        test_file_name = f"test_{file_path.name}"
        test_file_path = Path(verified_path) / "tests" / "unit" / test_file_name
        test_file_path.parent.mkdir(parents=True, exist_ok=True)
        
        test_content = f'''"""Generated unit tests for {file_path.name}"""
import pytest
import sys
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

'''
        
        for cls in analysis.get("classes", []):
            test_content += generate_unit_test(cls, str(file_path))
        
        for func in analysis.get("functions", []):
            if not func["name"].startswith("_"):
                test_content += generate_unit_test(func, str(file_path))
        
        with open(test_file_path, 'w') as f:
            f.write(test_content)
        
        return f"""âœ… Unit tests generated successfully

ğŸ“ Test file: {test_file_path}
ğŸ§ª Classes tested: {len(analysis.get('classes', []))}
âš¡ Functions tested: {len([f for f in analysis.get('functions', []) if not f['name'].startswith('_')])}

Next steps:
1. Review and customize generated tests
2. Run: pytest {test_file_path}
"""
        
    except Exception as e:
        logger.error(f"Test generation error: {e}")
        return f"âŒ Error generating tests: {str(e)}"

@mcp.tool()
async def generate_e2e_tests(repo_path: str = "", base_url: str = "", test_name: str = "app") -> str:
    """Generate Playwright E2E tests for web applications."""
    if not repo_path.strip():
        return "âŒ Error: Repository path is required"
    if not base_url.strip():
        return "âŒ Error: Base URL is required"
    
    logger.info(f"Generating E2E tests for: {base_url}")
    
    path_exists, verified_path = verify_path_exists(repo_path)
    if not path_exists:
        return f"âŒ Error: {verified_path}"
    
    repo = Path(verified_path)
    
    try:
        test_dir = repo / "tests" / "e2e"
        test_dir.mkdir(parents=True, exist_ok=True)
        
        test_content = generate_playwright_test(base_url, test_name)
        
        test_file = test_dir / f"test_{test_name}_e2e.py"
        with open(test_file, 'w') as f:
            f.write(test_content)
        
        conftest_content = '''"""Pytest configuration for E2E tests."""
import pytest
from playwright.sync_api import sync_playwright

@pytest.fixture(scope="session")
def browser():
    """Launch browser for E2E tests."""
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        yield browser
        browser.close()

@pytest.fixture(scope="function")
def page(browser):
    """Create a new page for each test."""
    context = browser.new_context(
        viewport={'width': 1920, 'height': 1080}
    )
    page = context.new_page()
    yield page
    page.close()
    context.close()

@pytest.fixture
def base_url():
    """Base URL for the application."""
    return "''' + base_url + '''"
'''
        
        conftest_file = test_dir / "conftest.py"
        with open(conftest_file, 'w') as f:
            f.write(conftest_content)
        
        return f"""âœ… E2E tests generated successfully

ğŸŒ Base URL: {base_url}
ğŸ“ Test file: {test_file}
âš™ï¸ Config file: {conftest_file}

Next steps:
1. Customize test scenarios
2. Run: pytest {test_file} --headed (to see browser)
"""
        
    except Exception as e:
        logger.error(f"E2E generation error: {e}")
        return f"âŒ Error generating E2E tests: {str(e)}"

@mcp.tool()
async def execute_tests(repo_path: str = "", test_path: str = "") -> str:
    """Execute pytest tests with coverage reporting."""
    if not repo_path.strip():
        return "âŒ Error: Repository path is required"
    
    path_exists, verified_path = verify_path_exists(repo_path)
    if not path_exists:
        return f"âŒ Error: {verified_path}"
    
    logger.info(f"Executing tests in: {verified_path}")
    
    success, result = run_pytest(verified_path, test_path)
    
    if not success:
        return f"âŒ Test execution error: {result.get('error', 'Unknown error')}"
    
    exit_code = result.get("exit_code", 1)
    stdout = result.get("stdout", "")
    
    passed = stdout.count(" passed")
    failed = stdout.count(" failed")
    
    coverage_match = re.search(r'TOTAL\s+\d+\s+\d+\s+(\d+)%', stdout)
    coverage_pct = coverage_match.group(1) if coverage_match else "N/A"
    
    status = "âœ…" if exit_code == 0 else "âš ï¸"
    
    return f"""{status} Test Execution Complete

ğŸ“Š Results:
- Passed: {passed}
- Failed: {failed}
- Coverage: {coverage_pct}%

ğŸ“„ Report: {result.get('report_file')}
ğŸ“ˆ Coverage: {result.get('coverage_file')}

{stdout[:2000]}
"""

@mcp.tool()
async def repair_failing_tests(repo_path: str = "", test_output: str = "") -> str:
    """Analyze test failures and provide repair suggestions."""
    if not repo_path.strip():
        return "âŒ Error: Repository path is required"
    
    logger.info(f"Analyzing test failures for: {repo_path}")
    
    if not test_output.strip():
        return "âš ï¸ No test output provided. Run execute_tests first to get failure details."
    
    try:
        failures = parse_test_failures(test_output)
        
        if not failures:
            return "âœ… No test failures detected - all tests passing!"
        
        result = f"ğŸ”§ Test Repair Analysis\n\n"
        result += f"Found {len(failures)} failing test(s)\n\n"
        
        for i, failure in enumerate(failures, 1):
            result += f"{i}. {failure.get('test', 'Unknown test')}\n"
            suggestions = generate_test_repair(failure)
            for suggestion in suggestions:
                result += f"   ğŸ’¡ {suggestion}\n"
            result += "\n"
        
        result += "\nğŸ”„ Recommended Actions:\n"
        result += "1. Review the specific assertions in failing tests\n"
        result += "2. Check if implementation changed without updating tests\n"
        result += "3. Verify test data and fixtures are correct\n"
        result += "4. Re-run tests after making corrections\n"
        
        return result
        
    except Exception as e:
        logger.error(f"Repair analysis error: {e}")
        return f"âŒ Error analyzing failures: {str(e)}"

@mcp.tool()
async def generate_github_workflow(repo_path: str = "", test_command: str = "pytest") -> str:
    """Generate GitHub Actions workflow for CI/CD testing."""
    if not repo_path.strip():
        return "âŒ Error: Repository path is required"
    
    path_exists, verified_path = verify_path_exists(repo_path)
    if not path_exists:
        return f"âŒ Error: {verified_path}"
    
    logger.info(f"Generating GitHub workflow for: {verified_path}")
    
    try:
        workflow_dir = Path(verified_path) / ".github" / "workflows"
        workflow_dir.mkdir(parents=True, exist_ok=True)
        
        workflow_content = f'''name: Autonomous QA Testing

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov playwright httpx
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        if [ -f backend/requirements.txt ]; then pip install -r backend/requirements.txt; fi
        playwright install chromium
        playwright install-deps chromium
    
    - name: Run tests with coverage
      run: |
        {test_command} --cov=. --cov-report=xml --cov-report=term
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: false
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results
        path: |
          .coverage
          coverage.xml
          htmlcov/
'''
        
        workflow_file = workflow_dir / "qa_testing.yml"
        with open(workflow_file, 'w') as f:
            f.write(workflow_content)
        
        return f"""âœ… GitHub Actions workflow generated

ğŸ“„ Workflow file: {workflow_file}

Features included:
- âœ“ Runs on push and pull requests
- âœ“ Python 3.11 environment
- âœ“ Pytest with coverage reporting
- âœ“ Playwright E2E testing
- âœ“ Codecov integration
- âœ“ Test artifact uploads

Next steps:
1. Commit workflow file to repository
2. Push to GitHub
3. Check Actions tab for test results
"""
        
    except Exception as e:
        logger.error(f"Workflow generation error: {e}")
        return f"âŒ Error generating workflow: {str(e)}"

@mcp.tool()
async def create_test_fix_pr(repo_url: str = "", test_output: str = "", fixes: str = "") -> str:
    """Create GitHub PR with automated test fixes from QA Council analysis."""
    if not repo_url.strip():
        return "âŒ Error: Repository URL is required"
    
    owner, repo = extract_github_info(repo_url)
    if not owner or not repo:
        return "âŒ Error: Invalid GitHub repository URL"
    
    if not GITHUB_TOKEN:
        return "âŒ Error: GITHUB_TOKEN not configured. Set it as an environment variable."
    
    logger.info(f"Creating test fix PR for {owner}/{repo}")
    
    try:
        # Get repo path
        repo_name = sanitize_repo_name(repo_url)
        repo_path = str(WORKSPACE_DIR / repo_name)
        
        # Create fix branch
        branch_name = f"qa-council/test-fixes-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
        
        # Parse fixes (expect JSON string)
        try:
            fix_list = json.loads(fixes) if fixes.strip() else []
        except:
            fix_list = []
        
        if fix_list:
            success, result = await create_test_fix_branch(repo_path, branch_name, fix_list)
            if not success:
                return f"âŒ Failed to create fix branch: {result}"
        
        # Create PR
        pr_title = "ğŸ¤– Automated Test Fixes from QA Council"
        pr_body = f"""## ğŸ¤– Automated Test Repair Analysis

This PR contains automated fixes suggested by the QA Council multi-agent system.

### ğŸ“Š Analysis Summary
{test_output[:500] if test_output else 'Test analysis completed'}

### ğŸ”§ Fixes Applied
{chr(10).join([f"- {fix.get('description', 'Test fix')}" for fix in fix_list]) if fix_list else '- Analysis and recommendations provided'}

### âœ… Next Steps
1. Review the changes carefully
2. Run tests locally: `pytest -v`
3. Merge if all tests pass

---
*Generated by QA Council Autonomous Testing System*
"""
        
        success, pr_url = await create_github_pr(owner, repo, pr_title, pr_body, branch_name)
        
        if success:
            return f"""âœ… Pull Request Created Successfully

ğŸ”— PR URL: {pr_url}
ğŸ“ Branch: {branch_name}
ğŸ¤– Generated by: QA Council

The PR includes:
- Automated test fixes
- Detailed analysis
- Repair recommendations

Review and merge when ready!
"""
        else:
            return f"âŒ Failed to create PR: {pr_url}"
        
    except Exception as e:
        logger.error(f"PR creation error: {e}")
        return f"âŒ Error creating PR: {str(e)}"

@mcp.tool()
async def orchestrate_full_qa_cycle(repo_url: str = "", branch: str = "main", base_url: str = "") -> str:
    """Execute complete QA lifecycle by calling all specialized agent tools in sequence - FIXED VERSION."""
    if not repo_url.strip():
        return "âŒ Error: Repository URL is required"
    
    logger.info(f"ğŸ¯ Starting Full QA Cycle with Council of Agents")
    results = []
    
    # AGENT 1: REPOSITORY AGENT
    results.append("=" * 70)
    results.append("ğŸ‘¤ AGENT 1: REPOSITORY AGENT")
    results.append("=" * 70)
    
    clone_result = await clone_repository(repo_url=repo_url, branch=branch)
    results.append(clone_result)
    
    if "âŒ" in clone_result:
        return "\n".join(results)
    
    repo_path = clone_result.split(": ")[1] if ": " in clone_result else f"/app/repos/{sanitize_repo_name(repo_url)}"
    
    # AGENT 2: INSPECTOR/ANALYZER AGENT
    results.append("\n" + "=" * 70)
    results.append("ğŸ‘¤ AGENT 2: INSPECTOR/ANALYZER AGENT")
    results.append("=" * 70)
    
    analysis_result = await analyze_codebase(repo_path=repo_path, file_pattern="*.py")
    results.append(analysis_result)
    
    # AGENT 3: TEST GENERATOR AGENT
    results.append("\n" + "=" * 70)
    results.append("ğŸ‘¤ AGENT 3: TEST GENERATOR AGENT")
    results.append("=" * 70)
    
    test_targets = ["backend/main.py", "database/database_setup.py", "prestart.py"]
    generated_count = 0
    
    for target in test_targets:
        gen_result = await generate_unit_tests(repo_path=repo_path, target_file=target)
        if "âœ…" in gen_result:
            generated_count += 1
        results.append(f"\nğŸ“ Target: {target}")
        results.append(gen_result[:300])
    
    if base_url.strip():
        e2e_result = await generate_e2e_tests(
            repo_path=repo_path,
            base_url=base_url,
            test_name="media_tracker"
        )
        results.append("\nğŸŒ E2E Tests:")
        results.append(e2e_result)
        generated_count += 1
    
    # AGENT 4: EXECUTOR AGENT
    results.append("\n" + "=" * 70)
    results.append("ğŸ‘¤ AGENT 4: EXECUTOR AGENT")
    results.append("=" * 70)
    
    exec_result = await execute_tests(repo_path=repo_path)
    results.append(exec_result)
    
    # AGENT 5: REPAIRER AGENT (if tests failed)
    if "failed" in exec_result.lower() or "âŒ" in exec_result:
        results.append("\n" + "=" * 70)
        results.append("ğŸ‘¤ AGENT 5: REPAIRER AGENT")
        results.append("=" * 70)
        
        repair_result = await repair_failing_tests(
            repo_path=repo_path,
            test_output=exec_result
        )
        results.append(repair_result)
    else:
        results.append("\nâ­ï¸  Agent 5 (Repairer) skipped - no failures detected")
    
    # AGENT 6: CI/CD AGENT
    results.append("\n" + "=" * 70)
    results.append("ğŸ‘¤ AGENT 6: CI/CD AGENT")
    results.append("=" * 70)
    
    workflow_result = await generate_github_workflow(
        repo_path=repo_path,
        test_command="pytest --cov=backend --cov-report=xml -v"
    )
    results.append(workflow_result)
    
    # FINAL SUMMARY
    results.append("\n" + "=" * 70)
    results.append("âœ… COUNCIL OF AGENTS - COMPLETE")
    results.append("=" * 70)
    results.append(f"""
ğŸ“Š Execution Summary:
  âœ… Repository Agent - Code cloned to {repo_path}
  âœ… Inspector Agent - Codebase analyzed
  âœ… Generator Agent - {generated_count} test suites created
  âœ… Executor Agent - Tests executed with coverage
  {"âœ… Repairer Agent - Failures analyzed" if "failed" in exec_result.lower() else "â­ï¸  Repairer Agent - Skipped (no failures)"}
  âœ… CI/CD Agent - GitHub Actions workflow generated

ğŸ¯ Next Steps:
  1. Review test files in {repo_path}/tests/
  2. Customize test assertions
  3. Commit .github/workflows/qa_testing.yml
  4. Push to GitHub to activate CI/CD
  5. Use create_test_fix_pr tool if fixes needed
    """)
    
    return "\n".join(results)

# === SERVER STARTUP ===
if __name__ == "__main__":
    logger.info("Starting Autonomous QA Testing Council MCP server (FIXED VERSION)...")
    
    if not GITHUB_TOKEN:
        logger.warning("GITHUB_TOKEN not set - PR creation and private repos will be unavailable")
    
    logger.info(f"Workspace directory: {WORKSPACE_DIR}")
    logger.info(f"Test results directory: {TEST_RESULTS_DIR}")
    logger.info(f"Coverage directory: {COVERAGE_DIR}")
    
    try:
        mcp.run(transport='stdio')
    except Exception as e:
        logger.error(f"Server error: {e}", exc_info=True)
        sys.exit(1)
